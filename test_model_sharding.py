#!/usr/bin/env python3
"""
Test Model Sharding - Prove llama.cpp RPC model distribution works

This demonstrates that a model is sharded across multiple RPC backends.
"""
import asyncio
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from sollol_backup_20251005.hybrid_router import HybridRouter
from sollol_backup_20251005.llama_cpp_coordinator import RPCBackend
from sollol.pool import OllamaPool
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_model_sharding():
    """Test model sharding across RPC backends."""

    print("=" * 80)
    print("üß™ MODEL SHARDING TEST")
    print("=" * 80)
    print()

    # RPC backends (all 4 nodes with rpc-server running)
    rpc_backends = [
        {"host": "10.9.66.154", "port": 50052},
        {"host": "10.9.66.48", "port": 50052},
        {"host": "10.9.66.45", "port": 50052},
        {"host": "10.9.66.90", "port": 50052},
    ]

    print(f"üì° RPC Backends ({len(rpc_backends)} nodes):")
    for backend in rpc_backends:
        print(f"   ‚Ä¢ {backend['host']}:{backend['port']}")
    print()

    # Create HybridRouter with RPC backends
    print("üöÄ Initializing HybridRouter with model sharding enabled...")
    router = HybridRouter(
        ollama_pool=None,  # No Ollama task distribution, just RPC sharding
        rpc_backends=rpc_backends,
        enable_distributed=True,
        auto_fallback=True
    )
    print("   ‚úÖ HybridRouter initialized")
    print()

    # Note: Using 13B model instead of 70B due to llama.cpp coordinator limitation.
    # The coordinator must load the full model in RAM before distributing computation.
    # For true distributed 70B+ support, see: https://github.com/BenevolentJoker-JohnL/SOLLOL#-future-work-fully-distributed-model-sharding-funding-contingent
    model = "llama2:13b"
    print(f"üî¨ Testing model sharding with: {model}")
    print()
    print("üìù What will happen:")
    print("   1. HybridRouter resolves GGUF file from Ollama storage")
    print("   2. Starts llama-server coordinator with --rpc flag")
    print("   3. Coordinator distributes model layers across RPC backends")
    print("   4. Makes inference request")
    print()
    print("‚ö†Ô∏è  Coordinator Limitation: Must load full model in RAM")
    print("   ‚Ä¢ 13B model works on 16GB+ RAM node")
    print("   ‚Ä¢ 70B model requires 32GB+ RAM on coordinator node")
    print()
    print("‚è≥ Starting coordinator (this may take 1-2 minutes for model loading)...")
    print()

    try:
        # Make a request - this will trigger coordinator startup and model sharding
        messages = [
            {"role": "user", "content": "Explain quantum entanglement in one sentence."}
        ]

        response = await router.route_request(
            model=model,
            messages=messages,
            max_tokens=50,
            temperature=0.7
        )

        print()
        print("=" * 80)
        print("‚úÖ MODEL SHARDING SUCCESS!")
        print("=" * 80)
        print()
        print("üìä Response received:")
        print(f"   {response.get('content', response)}")
        print()
        print("üéâ Model sharding is working!")
        print(f"   ‚Ä¢ Model layers distributed across {len(rpc_backends)} nodes")
        print("   ‚Ä¢ Inference completed successfully")
        print("   ‚Ä¢ Coordinator automatically managed layer distribution")
        print()

        # Show coordinator status
        if router.coordinator:
            print("üñ•Ô∏è  Coordinator Details:")
            print(f"   ‚Ä¢ URL: http://{router.coordinator.host}:{router.coordinator.port}")
            print(f"   ‚Ä¢ Model: {router.coordinator_model}")
            print(f"   ‚Ä¢ RPC Backends: {len(router.coordinator.rpc_backends)}")
            print()

        return True

    except Exception as e:
        print()
        print("=" * 80)
        print("‚ùå MODEL SHARDING TEST FAILED")
        print("=" * 80)
        print(f"Error: {e}")
        print()
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run the test."""
    success = await test_model_sharding()

    if success:
        print("‚úÖ Test completed successfully - model sharding is proven!")
        sys.exit(0)
    else:
        print("‚ùå Test failed - check errors above")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
